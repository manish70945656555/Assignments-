{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb161ad7-43d4-493a-a250-88d478d7b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Ans:-Gradient Boosting Regression is a variant of boosting that is used for regression problems. It combines weak \n",
    "regression models (often decision trees) in an additive manner to create a strong regression model. It iteratively fits\n",
    "the weak models to the negative gradients of the loss function, minimizing the residuals and improving the overall \n",
    "prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd66a0-3bd1-41fa-bd04-57ba95a5c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef198a1a-8e4a-4d40-a8fc-cd29ca296d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 299.6926491857575\n",
      "R-squared: 0.9077805109444612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the mean of the target values\n",
    "        y_pred = np.full_like(y, np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = y - y_pred\n",
    "            \n",
    "            # Fit a weak regression model (decision tree) to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the predictions with the weak model's predictions\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the weak model and its weight\n",
    "            self.trees.append(tree)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize predictions with zeros\n",
    "        y_pred = np.zeros(len(X))\n",
    "        \n",
    "        for tree, weight in zip(self.trees, self.weights):\n",
    "            # Add the weak model's predictions to the final predictions\n",
    "            y_pred += weight * tree.predict(X)\n",
    "            \n",
    "        return y_pred\n",
    "\n",
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5a6c6-8ff5-4bfe-9d8c-88eba821d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16dfd6d-07cb-42a9-bb36-6a5b3734a1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best Model Mean Squared Error: 30.014084727742453\n",
      "Best Model R-squared: 0.9951375878631904\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the mean of the target values\n",
    "        y_pred = np.full_like(y, np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = y - y_pred\n",
    "            \n",
    "            # Fit a weak regression model (decision tree) to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the predictions with the weak model's predictions\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the weak model and its weight\n",
    "            self.trees.append(tree)\n",
    "            self.weights.append(self.learning_rate)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize predictions with zeros\n",
    "        y_pred = np.zeros(len(X))\n",
    "        \n",
    "        for tree, weight in zip(self.trees, self.weights):\n",
    "            # Add the weak model's predictions to the final predictions\n",
    "            y_pred += weight * tree.predict(X)\n",
    "            \n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Calculate R-squared score\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Create and train the gradient boosting regressor with the best hyperparameters\n",
    "best_gb_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth']\n",
    ")\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using the best model\n",
    "y_pred_best = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"Best Model Mean Squared Error:\", mse_best)\n",
    "print(\"Best Model R-squared:\", r2_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576c848-69af-4c3a-b7da-c4c5fe24b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Ans:- In Gradient Boosting, a weak learner refers to a base model that performs slightly better than random guessing.\n",
    "It is typically a simple model with low complexity, such as a decision tree with limited depth or a linear model with \n",
    "few features. The weak learners are sequentially added to the ensemble to improve the overall performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1487284-bd75-44f7-be65-9ca6fd37a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Ans:-The intuition behind the Gradient Boosting algorithm is to combine multiple weak learners to create a powerful \n",
    "ensemble model. It works by iteratively adding weak learners to the ensemble, where each new learner focuses on correcting\n",
    "the mistakes made by the previous learners. The algorithm puts more emphasis on the instances that were poorly predicted\n",
    "in the previous iteration, allowing subsequent learners to focus on these challenging samples. By gradually reducing the\n",
    "errors, the ensemble of weak learners can learn complex relationships in the data and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba142b2-8e4d-4e50-bdc5-8a7624d006a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Ans:- The Gradient Boosting algorithm builds an ensemble of weak learners in a stage-wise manner. At each stage, it\n",
    "identifies the shortcomings of the current ensemble and adds a new weak learner to improve upon those deficiencies.\n",
    "The new weak learner is trained to minimize the errors made by the ensemble. By iteratively repeating this process, the\n",
    "algorithm gradually constructs an ensemble that can effectively predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99a7de-fd26-4dad-8de0-a3566c44fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "Ana:-The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm are as follows:\n",
    "\n",
    "1.Initialize the predictions: Start by initializing the predictions with a constant value, such as the mean of the target\n",
    "  variable.\n",
    "\n",
    "2.Compute the residuals: Calculate the difference between the actual target values and the current predictions. These\n",
    "  residuals represent the errors made by the current ensemble.\n",
    "\n",
    "3.Train a weak learner: Fit a weak learner (e.g., decision tree, linear model) to the residuals. The goal is to find a \n",
    "  model that can capture and correct the errors made by the current ensemble.\n",
    "\n",
    "4.Update the predictions: Multiply the predictions of the weak learner by a small learning rate \n",
    "  (to control the contribution of each weak learner) and add them to the current predictions. This update step reduces\n",
    "  the residuals and improves the overall predictions.\n",
    "\n",
    "5.Repeat steps 2-4: Iterate the process by recalculating the residuals based on the updated predictions and training a \n",
    "  new weak learner on the residuals. Continue this process until a predefined number of weak learners has been added or\n",
    "  until the desired performance is achieved.\n",
    "\n",
    "6.Generate the final prediction: Combine the predictions of all the weak learners in the ensemble to obtain the final \n",
    " prediction. This is typically done by summing the predictions from each weak learner, with each prediction weighted by\n",
    " a factor determined during training.\n",
    "\n",
    "By iteratively updating the predictions and adding weak learners to correct the errors, the Gradient Boosting algorithm\n",
    "constructs a strong ensemble model that can accurately predict the target variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
