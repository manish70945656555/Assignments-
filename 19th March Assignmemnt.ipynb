{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c7237-254e-41ff-9e1a-e4c1025d3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Ans:-Missing values in a dataset refer to the absence of a value for a particular variable or observation. They can occur for various reasons, such\n",
    "as data entry errors, non-response in surveys, or technical problems during data collection.\n",
    "\n",
    "It is essential to handle missing values because they can affect the quality of the analysis and the accuracy of the results. Some of the reasons \n",
    "why missing values are problematic are:\n",
    "\n",
    "1.They reduce the sample size, which can lead to biased or inefficient estimates.\n",
    "2.They can affect the validity of statistical tests, as missing data can create bias and reduce the power of the tests.\n",
    "3.They can affect the accuracy of predictive models, as missing values can lead to incomplete information and poor predictions.\n",
    "\n",
    "Some of the algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting algorithms.\n",
    "These algorithms can handle missing values by either ignoring the missing values or imputing them with a predicted value based on the available data. \n",
    "Other algorithms, such as linear regression, logistic regression, and neural networks, require complete data and may produce biased or incorrect \n",
    "results if missing values are not handled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0725e-e55e-4995-8f27-7e745e7040f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "Ans:-There are several techniques to handle missing data in a dataset. Here are some commonly used techniques with an example of how to implement\n",
    "them using Python:\n",
    "\n",
    "1.Deletion: This technique involves deleting rows or columns that contain missing values. However, this method can reduce the size of the dataset \n",
    "and may result in the loss of useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142bd9f0-ddb1-4c0e-a5dc-c494f106c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "data = {'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d94bbd-5e10-424d-8ec6-22d6e2bc95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.Mean/Mode/Median Imputation: This technique involves filling in the missing values with the mean, mode, or median of the respective feature.\n",
    "However, this method can introduce bias and distort the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7081ef2-e0e3-437f-8e39-9aa51c231ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B   C\n",
      "0  1.000000  5.0   9\n",
      "1  2.000000  6.5  10\n",
      "2  2.333333  6.5  11\n",
      "3  4.000000  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "data = {'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fill in missing values with mean\n",
    "df['A'].fillna(df['A'].mean(), inplace=True)\n",
    "df['B'].fillna(df['B'].mean(), inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c44887-5f04-4069-9ad9-817f73c61aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Forward/Backward Fill: This technique involves filling in the missing values with the previous or next value in the column. This method works\n",
    "well when the data has a temporal or sequential relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6938c331-c3d0-4174-acf8-1562e462a74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  5.0  10\n",
      "2  2.0  5.0  11\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "data = {'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fill in missing values with forward fill\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f9b1b-601a-4c44-ad0e-a079980e0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "Ans:-Imbalanced data refers to a situation where the classes in a classification problem are not represented equally in the dataset. Specifically,\n",
    "one class may have significantly fewer examples than another, leading to a class distribution that is skewed or imbalanced.\n",
    "\n",
    "For example, in a binary classification problem where the goal is to predict whether a customer will churn or not, the majority of customers might \n",
    "be retained, and only a small proportion will churn. If this dataset is used to train a model, the resulting model may be biased towards the\n",
    "majority class, leading to poor performance on the minority class.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to several issues:\n",
    "\n",
    "1.Poor Model Performance: Models trained on imbalanced data tend to perform poorly on the minority class, as they are biased towards the majority \n",
    "class. This can lead to poor accuracy, precision, recall, F1-score, and other performance metrics.\n",
    "\n",
    "2.Overfitting: Models trained on imbalanced data are prone to overfitting, as they tend to learn patterns that are specific to the majority class and \n",
    "may not generalize well to new data.\n",
    "\n",
    "3.Misclassification Costs: In many real-world scenarios, misclassifying the minority class can be more costly than misclassifying the majority class.\n",
    "For example, in medical diagnosis, misclassifying a patient with a rare disease as healthy can have severe consequences.\n",
    "\n",
    "To handle imbalanced data, several techniques can be used, such as resampling, using different evaluation metrics, cost-sensitive learning, and \n",
    "algorithm-specific approaches. It is important to handle imbalanced data to ensure that the resulting model is accurate and can generalize well to \n",
    "new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07e617-a457-4f1a-a720-0ce4b00e0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "Ans:-Up-sampling and down-sampling are techniques used to address imbalanced data in a dataset.\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class, typically by randomly removing instances from the majority class\n",
    "until a balance is achieved with the minority class. This technique can be useful when the dataset is large, and removing instances will not lead \n",
    "to significant information loss.\n",
    "\n",
    "Up-sampling involves increasing the number of instances in the minority class, typically by creating synthetic instances using techniques such as\n",
    "SMOTE (Synthetic Minority Over-sampling Technique). This technique can be useful when the dataset is small, and removing instances from the majority \n",
    "class is not feasible.\n",
    "\n",
    "For example, let's consider a dataset of credit card transactions, where the goal is to detect fraud. Suppose the dataset contains 100,000 \n",
    "transactions, of which only 1% are fraudulent. This is an imbalanced dataset since the number of fraudulent transactions is much smaller than the\n",
    "number of legitimate transactions.\n",
    "\n",
    "To handle this imbalance, we could down-sample the legitimate transactions to match the number of fraudulent transactions. Alternatively, we could \n",
    "up-sample the fraudulent transactions by creating synthetic transactions using SMOTE. In this case, up-sampling would be a better choice since we \n",
    "do not want to remove legitimate transactions, which could lead to a loss of information.\n",
    "\n",
    "In summary, up-sampling and down-sampling are techniques used to address imbalanced data in a dataset. The choice between the two depends on the \n",
    "size of the dataset and the importance of retaining information from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5d664-5d8d-47f2-b314-87c9b28f3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "Ans:-Data augmentation is a technique used to artificially increase the size of a dataset by creating new data from the existing data. The goal of\n",
    "data augmentation is to create new examples that are representative of the underlying distribution of the data, while also reducing overfitting by \n",
    "introducing variation into the training data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to address imbalanced datasets. The goal of SMOTE \n",
    "is to create new synthetic examples of the minority class by interpolating between existing examples.\n",
    "\n",
    "Here is how SMOTE works:\n",
    "\n",
    "For each example in the minority class, SMOTE selects k-nearest neighbors from the minority class.\n",
    "\n",
    "SMOTE then randomly selects one of the k-nearest neighbors and creates a synthetic example by interpolating between the selected example and the \n",
    "original example. This is done by selecting a random point along the line that connects the two examples.\n",
    "\n",
    "This process is repeated for each example in the minority class, creating a set of synthetic examples that are representative of the underlying\n",
    "distribution of the minority class.\n",
    "\n",
    "Here is an example of how SMOTE can be implemented in Python using the imblearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d71324-1390-4eeb-b66a-e26efd31a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# generate an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "                            weights=[0.1, 0.9], n_informative=3,\n",
    "                            n_redundant=1, flip_y=0, n_features=20,\n",
    "                            n_clusters_per_class=1, n_samples=1000,\n",
    "                            random_state=42)\n",
    "\n",
    "# perform SMOTE on the dataset\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687aaf34-467f-42f0-9b99-fad7cae62791",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this example, we first generate an imbalanced dataset with 1000 samples and 20 features, where the minority class has a weight of 0.1. We then \n",
    "use SMOTE to create new synthetic examples of the minority class, resulting in a balanced dataset with the same number of examples in each class.\n",
    "\n",
    "Overall, SMOTE is a powerful technique for addressing imbalanced datasets and can be used to improve the performance of machine learning models on \n",
    "imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6049c6a-7c61-4876-9845-aed490b5a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "Ans:-Outliers are data points that are significantly different from other data points in a dataset. These are observations that are unusually large \n",
    "or small when compared to the other data points in the dataset. Outliers can occur due to measurement error, experimental error, or natural variation\n",
    "in the data.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the results of a statistical analysis or machine learning model. \n",
    "Outliers can lead to incorrect estimates of statistical parameters, biased models, and reduced model performance. Outliers can also affect the\n",
    "interpretation of the results, leading to incorrect conclusions.\n",
    "\n",
    "Here are some reasons why it is essential to handle outliers in a dataset:\n",
    "\n",
    "1.Outliers can distort the distribution of the data: Outliers can affect the mean and standard deviation of a dataset, leading to a skewed distribution \n",
    "that is not representative of the true distribution of the data.\n",
    "\n",
    "2.Outliers can affect statistical analysis: Outliers can have a significant impact on statistical tests, leading to incorrect conclusions about the\n",
    "data.\n",
    "\n",
    "3.Outliers can lead to overfitting: Machine learning models are sensitive to outliers, and they can lead to overfitting, where the model is too \n",
    "complex and fits the training data too well, but fails to generalize to new data.\n",
    "\n",
    "There are various techniques to handle outliers, including removing outliers, transforming the data, or using robust statistical methods that are\n",
    "not sensitive to outliers.\n",
    "\n",
    "Overall, handling outliers is an essential step in data preprocessing and can improve the accuracy and reliability of statistical analysis and \n",
    "machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47122e39-fef7-4f65-a07d-8080b31dc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Ans:-There are several techniques that can be used to handle missing data in a dataset. Here are some of the most commonly used techniques:\n",
    "\n",
    "1.Deletion: One approach is to simply remove any data points with missing values from the dataset. This can be done in two ways:\n",
    "\n",
    "Listwise deletion: Delete any data points that have missing values in any of the variables.\n",
    "\n",
    "Pairwise deletion: Only delete the data points with missing values in the variables of interest, while retaining the other variables for the \n",
    "remaining data points.\n",
    "\n",
    "2.Imputation: Another approach is to impute missing values with estimated values. Some common imputation techniques include:\n",
    "\n",
    "Mean/median imputation: Replace missing values with the mean or median value of the variable.\n",
    "\n",
    "Regression imputation: Use regression analysis to estimate the missing values based on the relationship between the missing variable and other \n",
    "variables in the dataset.\n",
    "\n",
    "Multiple imputation: Use a statistical model to estimate multiple values for each missing value, creating several complete datasets for subsequent\n",
    "analysis.\n",
    "\n",
    "Ignore missing values: If the missing data is negligible, it may be acceptable to ignore it and proceed with the analysis.\n",
    "\n",
    "The choice of which technique to use will depend on the amount of missing data, the type of missingness, and the nature of the data. It is important\n",
    "to carefully consider the implications of each technique before choosing one.\n",
    "\n",
    "In the case of customer data, if the missing data is relatively small, imputation techniques such as mean/median imputation or regression imputation\n",
    "may be appropriate. However, if the missing data is extensive, deletion or ignoring the missing values may be necessary. It is essential to assess\n",
    "the impact of handling missing data on the results of the analysis and ensure that any chosen technique does not introduce bias or distort the\n",
    "analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e306625-2de3-4106-9e35-69488774cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "Ans:-Determining if the missing data is missing at random (MAR) or if there is a pattern to the missing data is crucial for selecting an appropriate\n",
    "technique to handle the missing data. Here are some strategies that can be used to identify if the missing data is MAR or non-random:\n",
    "\n",
    "1.Visual inspection: One approach is to plot the distribution of the data and visually inspect if there is a pattern to the missing data. \n",
    "For example, if the missing data occurs only for a specific group or at a particular time, it suggests that the missing data is not random.\n",
    "\n",
    "2.Correlation analysis: Correlation analysis can be used to identify any relationships between the missing data and other variables in the dataset.\n",
    "If there is a significant correlation between the missing data and other variables, it suggests that the missing data is not random.\n",
    "\n",
    "3.Hypothesis testing: Hypothesis testing can be used to test if the missing data is random or not. For example, a chi-square test can be used to\n",
    "test if the missing data is independent of the other variables in the dataset.\n",
    "\n",
    "4.Machine learning models: Machine learning models can also be used to predict missing values based on the other variables in the dataset. If the \n",
    "model performs well, it suggests that the missing data is random.\n",
    "\n",
    "In general, it is essential to assess the patterns of missing data in a dataset before selecting an appropriate strategy to handle the missing data.\n",
    "If the missing data is MAR, imputation techniques such as mean/median imputation or regression imputation may be appropriate. However, if the\n",
    "missing data is non-random, more advanced techniques such as multiple imputation or machine learning models may be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a37ff-6bf7-4eec-aaee-a737b9bea97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Ans:-Dealing with imbalanced datasets is a common problem in machine learning projects, and it is crucial to evaluate the performance of the model\n",
    "correctly. Here are some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "1.Confusion matrix: A confusion matrix is a table that summarizes the performance of a classification model. It contains four values: true positives\n",
    "(TP), false positives (FP), true negatives (TN), and false negatives (FN). It provides a clear picture of how well the model is classifying the data\n",
    "and can help identify where the model is making mistakes.\n",
    "\n",
    "2.ROC curve: The receiver operating characteristic (ROC) curve is a graphical representation of the trade-off between the true positive rate (TPR) \n",
    "and the false positive rate (FPR) of a classification model. It helps to identify the optimal threshold for the model.\n",
    "\n",
    "3.Precision-Recall curve: The precision-recall curve is another graphical representation of the trade-off between precision and recall of a \n",
    "classification model. It is particularly useful for imbalanced datasets where the focus is on correctly identifying the positive class.\n",
    "\n",
    "4.F1-score: The F1-score is a single metric that combines precision and recall to provide an overall measure of the model's performance. It is \n",
    "particularly useful for imbalanced datasets, where precision and recall are equally important.\n",
    "\n",
    "5.Resampling techniques: Resampling techniques such as oversampling and undersampling can be used to balance the dataset before training the model.\n",
    "Oversampling involves creating synthetic data points of the minority class, while undersampling involves removing data points from the majority \n",
    "class.\n",
    "\n",
    "6.Cost-sensitive learning: Cost-sensitive learning involves assigning different costs to different types of errors made by the model. For example, \n",
    "misclassifying a positive case may have a higher cost than misclassifying a negative case.\n",
    "\n",
    "In the case of a medical diagnosis project, where the dataset is imbalanced, a combination of these techniques can be used to evaluate the\n",
    "performance of the machine learning model. It is crucial to select the appropriate evaluation metric that focuses on correctly identifying the \n",
    "positive class, as well as using appropriate resampling techniques or cost-sensitive learning to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b024611-b210-40f8-8bcf-cb2271f8d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "Ans:-When dealing with imbalanced datasets, there are several methods that can be used to balance the dataset and down-sample the majority class.\n",
    "Here are some techniques that can be employed to balance the dataset and down-sample the majority class:\n",
    "\n",
    "1.Random undersampling: This method involves randomly removing instances from the majority class until it is balanced with the minority class. This \n",
    "technique can be effective, but it can also lead to the loss of useful information.\n",
    "\n",
    "2.Cluster-based undersampling: This method involves clustering the majority class and removing instances from each cluster until the dataset is \n",
    "balanced. This technique can be more effective than random undersampling because it ensures that the removed instances are not representative of \n",
    "the overall distribution of the majority class.\n",
    "\n",
    "3.Tomek links: Tomek links are pairs of samples from different classes that are close to each other, and removing the majority class sample can make \n",
    "the decision boundary clearer. This technique can be used to identify and remove noisy or ambiguous instances.\n",
    "\n",
    "4.NearMiss: NearMiss is a family of undersampling techniques that selects instances from the majority class that are closest to the minority class.\n",
    "NearMiss algorithms select samples from the majority class, such that they are the closest to the minority class samples.\n",
    "\n",
    "5.Hybrid methods: These methods combine both oversampling and undersampling techniques to balance the dataset. For example, one popular hybrid method \n",
    "is SMOTE combined with Tomek links, which removes noisy and borderline samples from the dataset while oversampling the minority class.\n",
    "\n",
    "In the case of a project where the dataset is unbalanced, with the bulk of customers reporting being satisfied, techniques like random undersampling\n",
    "or cluster-based undersampling can be employed to balance the dataset and down-sample the majority class. These techniques can remove instances from\n",
    "the majority class to create a more balanced dataset. However, it is important to evaluate the performance of the model on a separate validation \n",
    "set to ensure that the model is not underfitting on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff9982-23ae-482f-acec-f2d019d1435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "Ans:-When dealing with imbalanced datasets, there are several methods that can be used to balance the dataset and up-sample the minority class.\n",
    "Here are some techniques that can be employed to balance the dataset and up-sample the minority class:\n",
    "\n",
    "1.Random oversampling: This method involves randomly duplicating instances from the minority class until it is balanced with the majority class.\n",
    "This technique can be effective, but it can also lead to overfitting.\n",
    "\n",
    "2.Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a popular oversampling technique that generates synthetic samples by interpolating \n",
    "between existing minority class samples. It creates new minority class samples by taking random combinations of the k-nearest neighbors and using \n",
    "them to create new samples.\n",
    "\n",
    "3.Adaptive Synthetic Sampling (ADASYN): ADASYN is an extension of SMOTE that generates synthetic samples based on the density distribution of the \n",
    "minority class. It generates more synthetic samples in regions of the feature space where the density of the minority class is lower.\n",
    "\n",
    "4.Random undersampling: This method involves randomly removing instances from the majority class until it is balanced with the minority class. This\n",
    "technique can be effective, but it can also lead to the loss of useful information.\n",
    "\n",
    "5.Hybrid methods: These methods combine both oversampling and undersampling techniques to balance the dataset. For example, one popular hybrid \n",
    "method is SMOTE combined with Tomek links, which removes noisy and borderline samples from the dataset while oversampling the minority class.\n",
    "\n",
    "In the case of a project where the dataset is unbalanced with a low percentage of occurrences, techniques like SMOTE or ADASYN can be employed to \n",
    "balance the dataset and up-sample the minority class. These techniques can create synthetic samples that can improve the performance of the model.\n",
    "However, it is crucial to evaluate the performance of the model on a separate validation set to ensure that the model is not overfitting on the\n",
    "training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
