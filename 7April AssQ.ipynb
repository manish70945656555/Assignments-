{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e66d4-116a-47c3-ab83-95e71fc0f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\n",
    "Ans:-The relationship between polynomial functions and kernel functions in machine learning algorithms, specifically in \n",
    "Support Vector Machines (SVM), is that polynomial kernels are a type of kernel function used to transform data into higher\n",
    "-dimensional feature space.\n",
    "\n",
    "In SVM, the kernel function is responsible for mapping the input data from the original space to a higher-dimensional\n",
    "feature space, where the data becomes linearly separable. Polynomial kernels are one type of kernel function that use \n",
    "polynomial functions to perform this mapping. The polynomial kernel computes the similarity between two samples by \n",
    "evaluating the polynomial function on their feature vectors.\n",
    "\n",
    "The polynomial kernel function has the form K(x, y) = (gamma * <x, y> + coef0)^degree, where gamma, coef0, and degree are \n",
    "parameters that control the behavior of the kernel. The degree parameter determines the degree of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156e3e4-c576-4be7-b0a6-4d36bde243de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "Ans:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d80820a-eddf-47d8-ae33-b39de5624586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744bdf9-62c1-472a-afea-bcdef3f170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "Ans:-The value of epsilon (ε) determines the width of the epsilon-insensitive tube around the regression line.\n",
    "Increasing the value of epsilon will typically lead to more support vectors.\n",
    "\n",
    "Support vectors are data points that lie on the margin or within the epsilon-insensitive tube. They are critical for\n",
    "defining the regression line in SVR. When epsilon is small, the margin is narrow, and only data points very close to the\n",
    "regression line become support vectors. As epsilon increases, the margin widens, and more data points that lie within the\n",
    "expanded margin become support vectors.\n",
    "\n",
    "In general, increasing the value of epsilon in SVR can result in an increase in the number of support vectors because it\n",
    "allows more data points to be within the tolerance range. However, it is important to note that the specific effect of \n",
    "changing epsilon can depend on the dataset and the characteristics of the problem being modeled. It is advisable to\n",
    "experiment with different epsilon values to find the optimal value that balances model complexity and generalization\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6e01c-4aeb-4cfd-ba45-2dc64a4cbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Ans:-In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter \n",
    "can significantly affect the performance of the model. Here's an explanation of each parameter and how it influences SVR:\n",
    "\n",
    "1.Kernel Function:\n",
    "\n",
    "The kernel function determines the type of mapping used to transform the input data into a higher-dimensional feature \n",
    "space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "The choice of the kernel function depends on the nature of the data and the problem at hand. For example, the RBF kernel \n",
    "is often suitable for capturing complex nonlinear relationships, while the linear kernel works well for linearly separable data.\n",
    "It is important to choose an appropriate kernel function that aligns with the underlying patterns in the data.\n",
    "\n",
    "2.C Parameter:\n",
    "\n",
    "The C parameter (also known as the regularization parameter) controls the trade-off between the model's complexity and\n",
    "the amount of error allowed in the training data.\n",
    "A smaller C value puts more emphasis on achieving a larger margin, potentially allowing more training errors but leading\n",
    "to a simpler model. This can help prevent overfitting.\n",
    "On the other hand, a larger C value places more importance on classifying data points correctly, potentially leading to a more complex model that fits the training data more closely. This can increase the risk of overfitting.\n",
    "Adjusting the C parameter involves finding the right balance between model simplicity and accuracy. Cross-validation or grid search can be used to tune this parameter.\n",
    "\n",
    "3.Epsilon Parameter:\n",
    "\n",
    "The epsilon parameter (ε) determines the width of the epsilon-insensitive tube around the regression line. It defines the\n",
    "tolerance for errors in the training data.\n",
    "Data points that fall within the epsilon-insensitive tube are not considered as errors and do not contribute to the loss\n",
    "function.\n",
    "Increasing epsilon widens the tube and allows more data points to be within the tolerance range, potentially resulting in\n",
    "a larger number of support vectors and a more flexible model.\n",
    "Decreasing epsilon narrows the tube, making the model less tolerant to errors and potentially leading to fewer support\n",
    "vectors and a more constrained model.\n",
    "The choice of epsilon depends on the desired balance between model flexibility and sensitivity to errors in the training\n",
    "data.\n",
    "\n",
    "4.Gamma Parameter:\n",
    "\n",
    "The gamma parameter defines the influence of a single training example on the decision boundary.\n",
    "A small gamma value means that each training example has a large influence, leading to a more localized decision boundary.\n",
    "This can result in overfitting when the dataset is noisy or contains outliers.\n",
    "A large gamma value means that each training example has a smaller influence, resulting in a smoother decision boundary.\n",
    "This can help reduce overfitting and improve generalization.\n",
    "The optimal gamma value depends on the dataset and the specific problem. It is generally advisable to experiment with\n",
    "different gamma values to find the optimal setting.\n",
    "The choice of these parameters depends on the characteristics of the data and the problem being addressed. It often \n",
    "requires experimentation and fine-tuning. It is recommended to use techniques like cross-validation or grid search to \n",
    "find the optimal values that yield the best performance and generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b61c5-b53d-4235-9d59-c5013c125cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "1. Import the necessary libraries and load the dataset\n",
    "2. Split the dataset into training and testing setZ\n",
    "3. Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "4. Create an instance of the SVC classifier and train it on the training datW\n",
    "5. Use the trained classifier to predict the labels of the testing datW\n",
    "6. Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "   precision, recall, F1-scoreK\n",
    "7. Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "   improve its performanc_\n",
    "8. Train the tuned classifier on the entire dataseg\n",
    "9. Save the trained classifier to a file for future use.\n",
    "                                                                               \n",
    "Note:-You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
    "classification and has a sufficient number of features and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2b0cd6-129b-48a0-a8f4-9251c7cf1f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best parameters: {'C': 1, 'gamma': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Step 1: Import the necessary libraries and load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it on the training data\n",
    "svm = SVC()\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 7: Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "svm_tuned = SVC(**best_params)\n",
    "svm_tuned.fit(scaler.transform(iris.data), iris.target)\n",
    "\n",
    "# Step 9: Save the trained classifier to a file\n",
    "joblib.dump(svm_tuned, 'svm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32adf4-c71a-4bb4-be05-8dc87914060f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
